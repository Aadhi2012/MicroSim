This module solves a multiphase, multicomponent KKS-formulation of the phase-field equations.
The code is written using CUDA and OpenMPI, and can be employ multiple GPUs in a multi-node setting.
It has been tested using Nvidia Tesla V100s, Nvidia Tesla P100s, Nvidia Tesla K80s, Nvidia RTX 30x0, and Nvidia GTX 1650, with CUDA 11.x, 12.x and OpenMPI 4.0.x.

System requirements:

1. Nvidia HPC-SDK (v22.5 - v23.1)
    a. NVHPC includes CUDA-aware OpenMPI and CUDA.
    b. To install HPC-SDK 23.1, follow https://developer.nvidia.com/nvidia-hpc-sdk-231-downloads
       $ wget https://developer.download.nvidia.com/hpc-sdk/23.1/nvhpc_2023_231_Linux_x86_64_cuda_multi.tar.gz
       $ tar xpzf nvhpc_2023_231_Linux_x86_64_cuda_multi.tar.gz
       $ sudo nvhpc_2023_231_Linux_x86_64_cuda_multi/install
    c. Once installed to a directory of your convenience, export the NVHPC root directory and version by using
       $ export HPCSDK_VERSION=23.1; export NVHPC_ROOT=</path/to/directory/nvidia>
       $ export NVHPC_HOME=${NVHPC_ROOT}/hpc_sdk/Linux_x86_64/{HPCSDK_VERSION}
       $ export MPI_HOME=${NVHPC_HOME}/comm_libs/openmpi4/openmpi-4.0.5
    d. The Param supercomputers provide NVHPC package access through spack, and in some cases, through modules.
    e. Using the cuFFTMp library to run elastic calculations requires at Nvidia cards with SM_70 support or higher.

2. HDF5 (1.12.x+)
    a. Build HDF5 with parallel MPI support and the compiler set as mpicxx or mpicc. Instructions can be found in the HDF5 installation guide.
    b. https://www.hdfgroup.org/downloads/hdf5/source-code/
    c. Ideally, build HDF5 against the OpenMPI4 compilers that come with HPC-SDK. These can usually be found in ${NVHPC_ROOT}/hpc_sdk/Linux_x86_64/{HPCSDK_VERSION}/comm_libs.
    d. A sufficient set of commands for installation is
       $ CC=${MPI_HOME}/bin/mpicc CXX=${MPI_HOME}/bin/mpicxx ./configure --prefix=<where/you/want/to/install/HDF5////usr/local/is/preferred> --enable-cxx --enable-parallel --enable-unsupported
       $ make -j 4
       $ make install -j 4
       $ export HDF5_HOME=<prefix/from/the/earlier/configure/command>

3. GSL (2.7.1+)
    a. GSL can be installed by following the instructions listed in their website: https://www.gnu.org/software/gsl/
    b. If your GSL installation is not automatically linked to GCC, you can still include it by running
       $ export GSL_HOME = <GSL/installation/root>

To compile the solver, simply open a terminal in the base directory of the module and run the command 'make'.
$ make
To compile without cuFFTMp or HDF5 support,
$ make ENABLE_CUFFTMP=0 ENABLE_HDF5=0

For usage on the PARAM supercomputers, one can use the SLURM script (ParamJobScript.sh) and Makefile (Makefile_Param) that are included.
Since the packages may differ from platform to platform, some modifications to the above may be necessary.

To run the solver, use:
$ make run INPUT=<name_of_infile> FILLING=<name_of_filling_file> OUTPUT=<name_of_output_file> NPROCS=<number_of_processors>
or

$ LD_LIBRARY_PATH="${NVHPC_HOME}/math_libs/12.0/lib64/compat/nvshmem_2.6.0-1:${NVHPC_HOME}/math_libs/lib64:${NVHPC_HOME}/cuda/lib64:${LD_LIBRARY_PATH}" mpiexec -np <number_of_processors> ./microsim_kks_fd_cuda_mpi <name_of_infile> <name_of_filling_file> <name_of_output_file>

For .h5 files, with WRITEHDF5=1, output files need to be transformed in .xml format using the following command just above the DATA folder that is created upon execution
$ make write_xdmf
$ ./write_xdmf <name_of_infile> <name_of_output_file> <start_time> <end_time>

For ASCII/BINARY files in .vtk format the consolidated output files needs to be reconstructed out of separate processor files that are written in the DATA folder that is created upon execution
$ ./reconstruct <name_of_infile> <name_of_output_file> <Number of processes> <start_time> <end_time>

**********************************************
************** Additional Info ***************
**********************************************
-> This module can be used without CUDA-aware MPI and cuFFTMp, using CUDA (11.0+) and any MPI implementation (the CUDA-aware requirement need not be fulfilled). This can be run only by launching one process.
-> If the compilation returns errors stating that some header or library could not be found, check the PATH and LD_LIBRARY_PATH variables. They must respectively point to the bin and lib(64) directories 
   of MPI, CUDA, NVSHMEM (if using cuFFTMp), and HPC-SDK's math_libs directory.

 - GPU Phase-Field Developer Team @ IITH (Saurav Shenoy, Saswata Bhattacharya)

The following contributors are acknowledged
    Tushar Jogi
    Pankaj
    Hemanth Kumar Sandireddy
